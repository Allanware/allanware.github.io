<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Perceptual proxies for extracting averages in data visualizations | Wenxuan Zhao</title>
<meta name=keywords content="APAD"><meta name=description content="A paper a day: Yuan et al showed that when comparing averages across multiple data points, viewers use surprisingly primitive perceptual cues: the hierarchy of precision for perceptual encodings of data doesn&rsquo;t hold for comparison of averages of even pairs of data."><meta name=author content="Wenxuan Zhao"><link rel=canonical href=https://allanware.github.io/posts/2023/11/perceptual-proxies-for-extracting-averages-in-data-visualizations/><link crossorigin=anonymous href=/assets/css/stylesheet.5cfc680b1eeaeef9efbced92d46c2a9e876b72ee14fba85846afc4cff9e6e6f8.css integrity="sha256-XPxoCx7q7vnvvO2S1Gwqnodrcu4U+6hYRq/Ez/nm5vg=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script><link rel=icon href=https://allanware.github.io/favicons/favicon.png><link rel=icon type=image/png sizes=16x16 href=https://allanware.github.io/favicons/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://allanware.github.io/favicons/favicon-32x32.png><link rel=apple-touch-icon href=https://allanware.github.io/favicons/apple-touch-icon.png><link rel=mask-icon href=https://allanware.github.io/favicons/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://allanware.github.io/posts/2023/11/perceptual-proxies-for-extracting-averages-in-data-visualizations/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script async src="https://www.googletagmanager.com/gtag/js?id=G-Y7CDN6L98B"></script><script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-Y7CDN6L98B",{anonymize_ip:!1})}</script><meta property="og:title" content="Perceptual proxies for extracting averages in data visualizations"><meta property="og:description" content="A paper a day: Yuan et al showed that when comparing averages across multiple data points, viewers use surprisingly primitive perceptual cues: the hierarchy of precision for perceptual encodings of data doesn&rsquo;t hold for comparison of averages of even pairs of data."><meta property="og:type" content="article"><meta property="og:url" content="https://allanware.github.io/posts/2023/11/perceptual-proxies-for-extracting-averages-in-data-visualizations/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-11-07T12:42:22-06:00"><meta property="article:modified_time" content="2023-11-13T20:56:12-06:00"><meta property="og:site_name" content="Wenxuan Zhao"><meta name=twitter:card content="summary"><meta name=twitter:title content="Perceptual proxies for extracting averages in data visualizations"><meta name=twitter:description content="A paper a day: Yuan et al showed that when comparing averages across multiple data points, viewers use surprisingly primitive perceptual cues: the hierarchy of precision for perceptual encodings of data doesn&rsquo;t hold for comparison of averages of even pairs of data."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://allanware.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Perceptual proxies for extracting averages in data visualizations","item":"https://allanware.github.io/posts/2023/11/perceptual-proxies-for-extracting-averages-in-data-visualizations/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Perceptual proxies for extracting averages in data visualizations","name":"Perceptual proxies for extracting averages in data visualizations","description":"A paper a day: Yuan et al showed that when comparing averages across multiple data points, viewers use surprisingly primitive perceptual cues: the hierarchy of precision for perceptual encodings of data doesn\u0026rsquo;t hold for comparison of averages of even pairs of data.","keywords":["APAD"],"articleBody":"Intro The power of visualizations is constrained by the limitations and idiosyncrasies of the human visual system. - this is how they use the word “constraints”\nOne of the findings based on constraints is a hierarchy of judgment precision among visual features that encode data values - eg. spatial position encoding (eg. compare length on the common ground) \u003e length (stacked bar chart) \u003e area (treemap).\nThe catch: The precision of these encodings was determined for comparisons between single values, under the assumption that what is true for comparing single values is also true for multiple values. - researchers therefore often decompose graph reading into elementary perceptual processes.\nThere hypothesis is that the advantage of position encoding is not availble for comparisons involving larger sets: the visual system fails to use the most precise feature (position) and uses a less precise feature (length or even the area of the bars), because it may not be able to extract averages from multiple bar values; it may be restricted (or at least, irresistibly tempted) to considering sums only.\nExperiment 1 Stimuli Task: Participants judged which one of two variables had a higher value (1vs1) or higher mean (2vs2 and 6vs6).\nThree types of comparisons: 1 vs 1, 2 vs 2, and 6 vs 6.\nThree graph types: dot plots (position only), bar graph (position and spaial extent (length and area)), and misaligned bar graph (spaital extent only).\nThis graph shows the stimuli usign in both experiments: Design They used a 3 by 3 (3 comparison type by 3 graph type) with-in subject design to measure just noticeable differences (JND) using a staircase procedure. - A small JND means a participant can detect a small difference between two groups, reflecting higher perceptual discriminability. - During the staircase procedure, if participants correctly (or incorrectly) answered the previous trial, the difficulty (defined by the difference between the means of the y-axis values of each group) of the next trial increased (or decreased) by one level. - 12 levels of staircase. - The “ceiling” of the staircase could be determined by simulating random guess. People should do better than random guess. - A separate staircase was run for each of the nine conditions.\nThey also did several manipulations in generating stimuli to rule out the possibility that participants used statistical information other than the mean in their judgment. - for 6vs6, for example, they randomized which set had the larger mean with which set had the larger standard deviation, largest single value, and smallest single value.\nThe nine unique conditions were randomly presented in each block of nine trials. Each block was followed by a break that the participant could end by pressing a key. A new block of trials was then be shown in a random order, and these blocks continued until the end of the experiment. Once the 1vs1 conditions were complete, only 2vs2 and 6vs6 trials were shown.\nProcedure Participants were introduced to the experiment as a graph-reading task. They were told that the y-axis represented the time it took for a red and a blue team to finish a car racing game. Sometimes there was only one car within each team; sometimes there were multiple cars. Participants were told to judge which team on average took longer. For the misaligned bar graph condition, they were told explicitly that the length of the bars represented the time values.\nFor each trial: a fixation (500ms) + a left-right judgement on which side has higher average (2s, if no response after 2s, then marked incorrect and participants were reminded to response quicker).\n([25 trials for 1vs1] + [49 trials for 2vs2] + [49 trials for 6vs6]) × [3 graph types] = 369 trials plus a set of two practice trials. - why such imbalance\nResults To prove their hypothsis, they are able to find a significant interaction between comparison type and graph type: in 1vs1, the average JND differnece of misaligned bar graph is significantly higher than the other two. - These results indicate that participants had relied on the spatial position information when comparing individual values on normal bar graphs.\nIn contrast, for 2vs2 comparison, performance on the dot graphs was marginally significantly worse than on the normal bar graphs, but there was no significant performance difference between the normal bar graphs and the misaligned bar graphs. For 6vs6 comparisons, there was no significant difference among conditions.\nThe ANOVA analysis also revealed a main effect of comparison type: comparisons between single values significantly better than 2vs2, which were significantly better than 6vs6. They did not find a main effect of graph type.\nDiscussion In 1vs1, participants relied on the spatial position. In 2vs2 (pair comparison), participants had relied on the spatial extent of the bars rather than their spatial position in bar and misaligned bar graph conditions. For 6vs6 (mutivalue comparison), it is possible that participants treat entire set of bars as a single unit, summing the area of its shape envelope instead of the averaging the length of its bars. If so, this strategy should suffer for displays with an unequal number of bars between the groups. Thus, here comes the 2nd experiment.\nExperiment 2 Stimuli 2 new comparison types: 10vs10 and 6vs10. They also added small set size conditions (1vs1 and 2vs2) and used them as base-line measures to exclude outliers (whoever could not perform these baseline conditions within 3 SD from mean is excluded).\nDesign They used the staircase method with the three-down/one-up design: three correct answers in a row multiplied the staircase value by 0.75, and one incorrect answer multiplied the staircase value by 1.11. - These constants yielded a staircase that would converge on a JND equivalent to 68% accuracy.\nThe experiment comprised three blocks. In each block, 16 trials of one condition were presented consecutively. Following a break, this process was repeated until all conditions were seen. The second and third blocks were the same as the first, but the order of the conditions was randomized each time. The staircase of each condition in the second and third block was a continuation of the previous bock’s staircase. The experiment included a total of (5 set sizes) × (3 graph types) × (3 blocks) × (16 trials) = 720 trials per subject. Feedback was given for the first 16 trials for all conditions (the easiest part of the staircase) to ensure that subjects understood the task.\nResults Anova shows no significant effect for both set size and graph type and their interaction in 6vs6 and 10vs10 set size, so they combined these 2 into the same set size comparison condition and treated 6vs10 as the different set size comparison condition.\nThe ANOVA revealed a robust main effect between the forementioned two comparison types (same vs. different set size). The ANOVA did not detect a significant main effect of graph type.\nWhen there was unequal numbers of items between groups (different set sizes), a planned comparison between dot graphs and the normal bar graphs indicates that performance was significantly better on the dot graphs than on the normal bar graphs.\nDiscussion It is possible that, for the dots in the six versus ten condition, a center-of-mass proxy might be used for the dots, as opposed to a summed area proxy for the bars.\nConclusion \u0026 Future Directions The hierarchy of encoding precision of single-value comparisons (Cleveland \u0026 McGill, 1984) is typically assumed to extrapolate to more complex displays. The present results suggest that it does not, and that multivalue comparisons can rely on surprisingly different perceptual proxies for the desired statistics.\nWhy did participants appear to rely on sums instead of averages? Their result is particularly surprising given the extensive evidence that the visual system can compute visual statistics across basic features. This incongruent result may provide a case of use-inspired basic research (Shneiderman, 2015), leading visual cognition researchers to find the limiting factors to such statistical extraction.\nThe visual system is biased to deal with whole objects, rather than the features or parts of the objects, When asked to track multiple moving objects, observers have extreme difficulty tracking just one end of a moving bar, reflexively snapping their attention to the whole bar instead, and a similar reflex may lead viewers to bias recall of values plotted in bar graphs toward the center of the bar, instead of the top edge (with-in-the-bar-bias). In the current study, although spatial position might be the most relevant feature on a bar graph for comparing the averages between groups of values, viewers seem to be incapable of ignoring the spatial extent (length or area) information, and select the whole bars as the unit of attention.\n","wordCount":"1442","inLanguage":"en","datePublished":"2023-11-07T12:42:22-06:00","dateModified":"2023-11-13T20:56:12-06:00","author":{"@type":"Person","name":"Wenxuan Zhao"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://allanware.github.io/posts/2023/11/perceptual-proxies-for-extracting-averages-in-data-visualizations/"},"publisher":{"@type":"Organization","name":"Wenxuan Zhao","logo":{"@type":"ImageObject","url":"https://allanware.github.io/favicons/favicon.png"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://allanware.github.io/ accesskey=h title="Wenxuan Zhao (Alt + H)">Wenxuan Zhao</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li><li><a href=https://allanware.github.io/zh/ title=中文 aria-label=中文>Zh</a></li></ul></div></div><ul id=menu><li><a href=https://allanware.github.io/ title=Home><span>Home</span></a></li><li><a href=https://allanware.github.io/about/ title=About><span>About</span></a></li><li><a href=https://allanware.github.io/life/ title=Life><span>Life</span></a></li><li><a href=https://allanware.github.io/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://allanware.github.io/archives/ title=Archives><span>Archives</span></a></li><li><a href=https://allanware.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://allanware.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://allanware.github.io/posts/>Posts</a></div><h1 class=post-title>Perceptual proxies for extracting averages in data visualizations</h1><div class=post-meta><span title='2023-11-07 12:42:22 -0600 -0600'>November 7, 2023</span>&nbsp;·&nbsp;<span title='2023-11-13 20:56:12 -0600 -0600'>(updated November 13, 2023)</span>&nbsp;·&nbsp;Wenxuan Zhao</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#intro aria-label=Intro>Intro</a></li><li><a href=#experiment-1 aria-label="Experiment 1">Experiment 1</a><ul><li><a href=#stimuli aria-label=Stimuli>Stimuli</a></li><li><a href=#design aria-label=Design>Design</a></li><li><a href=#procedure aria-label=Procedure>Procedure</a></li><li><a href=#results aria-label=Results>Results</a></li><li><a href=#discussion aria-label=Discussion>Discussion</a></li></ul></li><li><a href=#experiment-2 aria-label="Experiment 2">Experiment 2</a><ul><li><a href=#stimuli-1 aria-label=Stimuli>Stimuli</a></li><li><a href=#design-1 aria-label=Design>Design</a></li><li><a href=#results-1 aria-label=Results>Results</a></li><li><a href=#discussion-1 aria-label=Discussion>Discussion</a></li></ul></li><li><a href=#conclusion--future-directions aria-label="Conclusion &amp;amp; Future Directions">Conclusion & Future Directions</a></li></ul></div></details></div><div class=post-content><h2 id=intro>Intro<a hidden class=anchor aria-hidden=true href=#intro>#</a></h2><p>The power of visualizations is constrained by the limitations and idiosyncrasies of the human visual system.
- this is how they use the word &ldquo;constraints&rdquo;</p><p>One of the findings based on constraints is a hierarchy of judgment precision among visual features that encode data values
- eg. spatial position encoding (eg. compare length on the common ground) > length (stacked bar chart) > area (treemap).</p><p>The catch: The precision of these encodings was determined for comparisons between <em>single</em> values, under the assumption that what is true for comparing single values is also true for multiple values.
- researchers therefore often decompose graph reading into elementary perceptual processes.</p><p>There hypothesis is that the advantage of position encoding is not availble for comparisons involving larger sets: the visual system fails to use the most precise feature (position) and uses a less precise feature (length or even the area of the bars), because it may not be able to extract averages from multiple bar values; it may be restricted (or at least, irresistibly tempted) to considering sums only.</p><h2 id=experiment-1>Experiment 1<a hidden class=anchor aria-hidden=true href=#experiment-1>#</a></h2><h3 id=stimuli>Stimuli<a hidden class=anchor aria-hidden=true href=#stimuli>#</a></h3><p>Task: Participants judged which one of two variables had a higher value (1vs1) or higher mean (2vs2 and 6vs6).</p><p>Three types of comparisons: 1 vs 1, 2 vs 2, and 6 vs 6.</p><p>Three graph types: dot plots (position only), bar graph (position and spaial extent (length and area)), and misaligned bar graph (spaital extent only).</p><p>This graph shows the stimuli usign in both experiments:
<img loading=lazy src=fig3.png alt=Fig3></p><h3 id=design>Design<a hidden class=anchor aria-hidden=true href=#design>#</a></h3><p>They used a 3 by 3 (3 comparison type by 3 graph type) with-in subject design to measure just noticeable differences (JND) using a staircase procedure.
- A small JND means a participant can detect a small difference between two groups, reflecting higher perceptual discriminability.
- During the staircase procedure, if participants correctly (or incorrectly) answered the previous trial, the difficulty (defined by the difference between the means of the y-axis values of each group) of the next trial increased (or decreased) by one level.
- 12 levels of staircase.
- The &ldquo;ceiling&rdquo; of the staircase could be determined by simulating random guess. People should do better than random guess.
- A separate staircase was run for each of the nine conditions.</p><p>They also did several manipulations in generating stimuli to rule out the possibility that participants used statistical information other than the mean in their judgment.
- for 6vs6, for example, they randomized which set had the larger mean with which set had the larger standard deviation, largest single value, and smallest single value.</p><p>The nine unique conditions were randomly presented in each block of nine trials. Each block was followed by a break that the participant could end by pressing a key. A new block of trials was then be shown in a random order, and these blocks continued until the end of the experiment. Once the 1vs1 conditions were complete, only 2vs2 and 6vs6 trials were shown.</p><h3 id=procedure>Procedure<a hidden class=anchor aria-hidden=true href=#procedure>#</a></h3><p>Participants were introduced to the experiment as a graph-reading task. They were told that the y-axis represented the time it took for a red and a blue team to finish a car racing game. Sometimes there was only one car within each team; sometimes there were multiple cars. Participants were told to judge which team on average took longer. For the misaligned bar graph condition, they were told explicitly that the length of the bars represented the time values.</p><p>For each trial: a fixation (500ms) + a left-right judgement on which side has higher average (2s, if no response after 2s, then marked incorrect and participants were reminded to response quicker).</p><p>([25 trials for 1vs1] + [49 trials for 2vs2] + [49 trials for 6vs6]) × [3 graph types] = 369 trials plus a set of two practice trials.
- why such imbalance</p><h3 id=results>Results<a hidden class=anchor aria-hidden=true href=#results>#</a></h3><p>To prove their hypothsis, they are able to find a significant interaction between comparison type and graph type: in 1vs1, the average JND differnece of misaligned bar graph is significantly higher than the other two.
- These results indicate that participants had relied on the spatial position information when comparing individual values on normal bar graphs.</p><p>In contrast, for 2vs2 comparison, performance on the dot graphs was marginally significantly worse than on the normal bar graphs, but there was no significant performance difference between the normal bar graphs and the misaligned bar graphs. For 6vs6 comparisons, there was no significant difference among conditions.</p><p>The ANOVA analysis also revealed a main effect of comparison type: comparisons between single values significantly better than 2vs2, which were significantly better than 6vs6. They did not find a main effect of graph type.</p><h3 id=discussion>Discussion<a hidden class=anchor aria-hidden=true href=#discussion>#</a></h3><p>In 1vs1, participants relied on the spatial position.
In 2vs2 (pair comparison), participants had relied on the spatial extent of the bars rather than their spatial position in bar and misaligned bar graph conditions.
For 6vs6 (mutivalue comparison), it is possible that participants treat entire set of bars as a single unit, summing the area of its shape envelope instead of the averaging the length of its bars. If so, this strategy should suffer for displays with an unequal number of bars between the groups. Thus, here comes the 2nd experiment.</p><h2 id=experiment-2>Experiment 2<a hidden class=anchor aria-hidden=true href=#experiment-2>#</a></h2><h3 id=stimuli-1>Stimuli<a hidden class=anchor aria-hidden=true href=#stimuli-1>#</a></h3><p>2 new comparison types: 10vs10 and 6vs10. They also added small set size conditions (1vs1 and 2vs2) and used them as base-line measures to exclude outliers (whoever could not perform these baseline conditions within 3 SD from mean is excluded).</p><h3 id=design-1>Design<a hidden class=anchor aria-hidden=true href=#design-1>#</a></h3><p>They used the staircase method with the three-down/one-up design: three correct answers in a row multiplied the staircase value by 0.75, and one incorrect answer multiplied the staircase value by 1.11.
- These constants yielded a staircase that would converge on a JND equivalent to 68% accuracy.</p><p>The experiment comprised three blocks. In each block, 16 trials of one condition were presented consecutively. Following a break, this process was repeated until all conditions were seen. The second and third blocks were the same as the first, but the order of the conditions was randomized each time. The staircase of each condition in the second and third block was a continuation of the previous bock’s staircase. The experiment included a total of (5 set sizes) × (3 graph types) × (3 blocks) × (16 trials) = 720 trials per subject. Feedback was given for the first 16 trials for all conditions (the easiest part of the staircase) to ensure that subjects understood the task.</p><h3 id=results-1>Results<a hidden class=anchor aria-hidden=true href=#results-1>#</a></h3><p>Anova shows no significant effect for both set size and graph type and their interaction in 6vs6 and 10vs10 set size, so they combined these 2 into the <em>same set size</em> comparison condition and treated 6vs10 as the <em>different set size</em> comparison condition.</p><p>The ANOVA revealed a robust main effect between the forementioned two comparison types (same vs. different set size). The ANOVA did not detect a significant main effect of graph type.</p><p>When there was unequal numbers of items between groups (different set sizes), a planned comparison between dot graphs and the normal bar graphs indicates that performance was significantly better on the dot graphs than on the normal bar graphs.</p><h3 id=discussion-1>Discussion<a hidden class=anchor aria-hidden=true href=#discussion-1>#</a></h3><p>It is possible that, for the dots in the six versus ten condition, a center-of-mass proxy might be used for the dots, as opposed to a summed area proxy for the bars.</p><h2 id=conclusion--future-directions>Conclusion & Future Directions<a hidden class=anchor aria-hidden=true href=#conclusion--future-directions>#</a></h2><p>The hierarchy of encoding precision of single-value comparisons (Cleveland & McGill, 1984) is typically assumed to extrapolate to more complex displays. The present results suggest that it does not, and that multivalue comparisons can rely on surprisingly different perceptual proxies for the desired statistics.</p><p>Why did participants appear to rely on sums instead of averages? Their result is particularly surprising given the extensive evidence that the visual system can compute visual statistics across basic features. This incongruent result may provide a case of use-inspired basic research (Shneiderman, 2015), leading visual cognition researchers to find the limiting factors to such statistical extraction.</p><p>The visual system is biased to deal with whole objects, rather than the features or parts of the objects, When asked to track multiple moving objects, observers have extreme difficulty tracking just one end of a moving bar, reflexively snapping their attention to the whole bar instead, and a similar reflex may lead viewers to bias recall of values plotted in bar graphs toward the center of the bar, instead of the top edge (with-in-the-bar-bias). In the current study, although spatial position might be the most relevant feature on a bar graph for comparing the averages between groups of values, viewers seem to be incapable of ignoring the spatial extent (length or area) information, and select the whole bars as the unit of attention.</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://allanware.github.io/tags/apad/>APAD</a></li></ul><nav class=paginav><a class=prev href=https://allanware.github.io/posts/2023/11/the-within-the-bar-bias/><span class=title>« Prev</span><br><span>The within-the-bar bias</span>
</a><a class=next href=https://allanware.github.io/posts/2023/11/biased-average-position-estimates-in-line-and-bar-graphs/><span class=title>Next »</span><br><span>Biased Average Position Estimates in Line and Bar Graphs</span></a></nav></footer><script src=https://utteranc.es/client.js repo=Allanware/allanware.github.io issue-term=pathname theme=preferred-color-scheme crossorigin=anonymous async></script></article></main><footer class=footer><span>&copy; 2023 <a href=https://allanware.github.io/>Wenxuan Zhao</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>